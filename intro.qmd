# Introducción {#sec-intro}

```{r}
#| results: "asis"
#| echo: false
source("_common.R")
```

La ciencia de datos es una disciplina apasionante que le permite transformar datos sin procesar en comprensión, perspectiva y conocimiento.
El objetivo de "R para la Ciencia de Datos" es ayudarlo a aprender las herramientas más importantes en R que le permitirán hacer ciencia de datos de manera eficiente y reproducible.
Después de leer este libro, tendrá las herramientas para abordar una amplia variedad de desafíos de ciencia de datos utilizando las mejores partes de R.

## Lo que vas a aprender

La ciencia de datos es un campo amplio, y no hay forma de que puedas dominarlo todo leyendo un solo libro.
Este libro tiene como objetivo brindarle una base sólida en las herramientas más importantes y el conocimiento suficiente para encontrar los recursos para aprender más cuando sea necesario.
Nuestro modelo de las herramientas necesarias en un proyecto típico de ciencia de datos se parece a @fig-ds-diagram.

```{r}
#| label: fig-ds-diagram
#| echo: false
#| fig-cap: >
#|   En nuestro modelo del proceso de ciencia de datos, comienza con la importación de datos
#|   y ordenando. Luego, comprende sus datos con un ciclo iterativo de
#|   transformar, visualizar y modelar. Terminas el proceso 
#|   comunicando sus resultados a otros humanos.
#| fig-alt: >
#|   Un diagrama que muestra el ciclo de la ciencia de datos: Importar -> Ordenar -> Comprender 
#|   (que tiene las fases Transformar -> Visualizar -> Modelar en un ciclo) -> 
#|   Comunicar. Rodeando todo esto está Comunicar.
#| out.width: NULL

knitr::include_graphics("diagrams/data-science/base.png", dpi = 270)
```

Primero, debe **importar** sus datos a R.
Esto generalmente significa que toma datos almacenados en un archivo, base de datos o interfaz de programación de aplicaciones web (API) y los carga en un marco de datos en R.
Si no puede llevar sus datos a R, ¡no puede hacer ciencia de datos en ellos!

Una vez que haya importado sus datos, es una buena idea **ordenarlos**.
Ordenar sus datos significa almacenarlos en una forma consistente que coincida con la semántica del conjunto de datos con la forma en que se almacenan.
En resumen, cuando sus datos están ordenados, cada columna es una variable y cada fila es una observación.
Los datos ordenados son importantes porque una estructura coherente le permite centrar sus esfuerzos en responder preguntas sobre los datos, sin luchar para obtener los datos en la forma correcta para diferentes funciones.

Una vez que tenga datos ordenados, el próximo paso común es **transformarlos**.
La transformación incluye la reducción de las observaciones de interés (como todas las personas en una ciudad o todos los datos del último año), la creación de nuevas variables que son funciones de las variables existentes (como la velocidad de cálculo a partir de la distancia y el tiempo) y el cálculo de un conjunto de estadísticas de resumen.
(como la mediana o la media).
Juntos, ordenar y transformar se denominan **disputar** (o manipular) porque obtener sus datos en una forma en la que es natural trabajar con ellos a menudo se siente como una pelea.

Una vez que tenga datos ordenados con las variables que necesita, hay dos motores principales de generación de conocimiento: visualización y modelado.
Estos tienen fortalezas y debilidades complementarias, por lo que cualquier análisis real iterará entre ellos muchas veces.

**La visualización** es una actividad fundamentalmente humana.
Una buena visualización le mostrará cosas que no esperaba o le generará nuevas preguntas sobre los datos.
Una buena visualización también podría indicar que está haciendo la pregunta incorrecta o que necesita recopilar datos diferentes.
Las visualizaciones pueden sorprenderlo y no se escalan particularmente bien porque requieren que un ser humano las interprete.

**Los modelos** son herramientas complementarias a la visualización.
Una vez que haya hecho sus preguntas lo suficientemente precisas, puede usar un modelo para responderlas.
Los modelos son una herramienta fundamentalmente matemática o computacional, por lo que generalmente se escalan bien.
¡Incluso cuando no lo hacen, por lo general es más barato comprar más computadoras que comprar más cerebros!
Pero todo modelo hace suposiciones y, por su propia naturaleza, un modelo no puede cuestionar sus propias suposiciones.
Eso significa que un modelo no puede sorprenderte fundamentalmente.

El último paso de la ciencia de datos es la **comunicación**, una parte absolutamente crítica de cualquier proyecto de análisis de datos.
No importa qué tan bien sus modelos y visualización lo hayan llevado a comprender los datos, a menos que también pueda comunicar sus resultados a otros.

Alrededor de todas estas herramientas está la **programación**.
La programación es una herramienta transversal que utiliza en casi todas las partes de un proyecto de ciencia de datos.
No necesita ser un programador experto para ser un científico de datos exitoso, pero aprender más sobre programación vale la pena porque convertirse en un mejor programador le permite automatizar tareas comunes y resolver nuevos problemas con mayor facilidad.

Usará estas herramientas en todos los proyectos de ciencia de datos, pero no son suficientes para la mayoría de los proyectos.
Hay una regla aproximada de 80-20 en juego; puede abordar aproximadamente el 80 % de cada proyecto con las herramientas que aprenderá en este libro, pero necesitará otras herramientas para abordar el 20 % restante.
A lo largo de este libro, le indicaremos los recursos donde puede obtener más información.

## Cómo está organizado este libro

La descripción anterior de las herramientas de la ciencia de datos está organizada aproximadamente según el orden en que las usa en un análisis (aunque, por supuesto, las repetirá varias veces).
En nuestra experiencia, sin embargo, aprender a ingerir y ordenar datos primero no es óptimo porque el 80 % del tiempo es rutinario y aburrido, y el otro 20 % del tiempo es raro y frustrante.
¡Ese es un mal lugar para comenzar a aprender un nuevo tema!
En su lugar, comenzaremos con la visualización y transformación de datos que ya se han importado y ordenado.
De esa manera, cuando ingiere y ordena sus propios datos, su motivación se mantendrá alta porque sabe que el esfuerzo vale la pena.

Dentro de cada capítulo, tratamos de adherirnos a un patrón similar: comience con algunos ejemplos motivadores para que pueda ver el panorama general y luego profundice en los detalles.
Cada sección del libro está emparejada con ejercicios para ayudarte a practicar lo que has aprendido.
Aunque puede ser tentador saltarse los ejercicios, no hay mejor manera de aprender que practicar con problemas reales.

## Lo que no aprenderás

Hay varios temas importantes que este libro no cubre.
Creemos que es importante concentrarse despiadadamente en lo esencial para que pueda ponerse en marcha lo más rápido posible.
Eso significa que este libro no puede cubrir todos los temas importantes.

### Modelado

Para obtener más información sobre el modelado, le recomendamos encarecidamente [Tidy Modeling with R](https://www.tmwr.org) por nuestros colegas Max Kuhn y Julia Silge.
Este libro le enseñará la familia de paquetes tidymodels que, como puede adivinar por el nombre, comparten muchas convenciones con los paquetes tidyverse que usamos en este libro.

### Big data

Este libro se enfoca orgullosamente y principalmente en pequeños conjuntos de datos en memoria.
Este es el lugar correcto para comenzar porque no puede abordar big data a menos que tenga experiencia con small data.
Las herramientas que aprende en la mayoría de este libro manejarán fácilmente cientos de megabytes de datos y, con un poco de cuidado, normalmente puede usarlas para trabajar con 1-2 Gb de datos.

Dicho esto, el libro también aborda la extracción de datos de las bases de datos y de los archivos de parquet, las cuales son soluciones comúnmente utilizadas para almacenar big data.

Sin embargo, si trabaja habitualmente con datos más grandes (por ejemplo, de 10 a 100 Gb), debe obtener más información sobre \[data.table\] (https://github.com/Rdatatable/data.table).
Este libro no enseña data.table porque tiene una interfaz muy concisa que ofrece menos pistas lingüísticas, lo que hace que sea más difícil de aprender.
Sin embargo, la recompensa por el rendimiento bien vale la pena el esfuerzo requerido para aprenderlo si está trabajando con grandes cantidades de datos.

Si sus datos son más grandes que esto, considere cuidadosamente si su problema de datos grandes es en realidad un problema de datos pequeños disfrazado.
Si bien el conjunto completo de datos puede ser grande, a menudo, los datos necesarios para responder una pregunta específica son pequeños.
Es posible que pueda encontrar un subconjunto, una submuestra o un resumen que quepa en la memoria y aún le permita responder la pregunta que le interesa.
El desafío aquí es encontrar los datos pequeños correctos, lo que a menudo requiere mucha iteración.

Otra posibilidad es que su gran problema de datos sea en realidad una gran cantidad de pequeños problemas de datos disfrazados.
Cada problema individual puede caber en la memoria, pero tienes millones de ellos.
Por ejemplo, es posible que desee ajustar un modelo a cada persona en su conjunto de datos.
Esto sería trivial si tuviera solo 10 o 100 personas; en cambio, tienes un millón.
Afortunadamente, cada problema es independiente de los demás (una configuración que a veces se llama vergonzosamente paralela), por lo que solo necesita un sistema (como [Hadoop](https://hadoop.apache.org/) o [Spark](https://spark.apache.org/)) que le permite enviar diferentes conjuntos de datos a diferentes computadoras para su procesamiento.
Una vez que haya descubierto cómo responder a su pregunta para un solo subconjunto utilizando las herramientas descritas en este libro, puede aprender nuevas herramientas como **sparklyr** para resolverlo para el conjunto de datos completo.

### Python, Julia, y amigos

En este libro, no aprenderá nada sobre Python, Julia o cualquier otro lenguaje de programación útil para la ciencia de datos.
Esto no se debe a que pensemos que estas herramientas son malas.
¡No lo son!
Y en la práctica, la mayoría de los equipos de ciencia de datos usan una combinación de lenguajes, a menudo al menos R y Python.

Sin embargo, creemos firmemente que es mejor dominar una herramienta a la vez.
Mejorará más rápido si se sumerge profundamente en lugar de dispersarse en muchos temas.
Esto no significa que solo debas saber una cosa, solo que generalmente aprenderás más rápido si te apegas a una cosa a la vez.
Debe esforzarse por aprender cosas nuevas a lo largo de su carrera, pero asegúrese de que su comprensión sea sólida antes de pasar a la siguiente cosa emocionante.

Creemos que R es un excelente lugar para comenzar su viaje de ciencia de datos porque es un entorno diseñado desde cero para respaldar la ciencia de datos.
R no es solo un lenguaje de programación; también es un entorno interactivo para hacer ciencia de datos.
Para respaldar la interacción, R es un lenguaje mucho más flexible que muchos de sus pares.
Esta flexibilidad tiene sus desventajas, pero la gran ventaja es lo fácil que es tener un código estructurado como el problema que intenta resolver para partes específicas del proceso de ciencia de datos.
Estos minilenguajes lo ayudan a pensar en los problemas como un científico de datos al mismo tiempo que respaldan una interacción fluida entre su cerebro y la computadora.

## Requisitos previos

Hemos hecho algunas suposiciones sobre lo que ya sabe para aprovechar al máximo este libro.
En general, debe tener conocimientos numéricos, y es útil si ya tiene algo de experiencia en programación.
Si nunca ha programado antes, es posible que encuentre que [Hands on Programming with R](https://rstudio-education.github.io/hopr/) por Garrett puede ser un valioso complemento de este libro.

Necesita cuatro cosas para ejecutar el código de este libro: R, RStudio, una colección de paquetes de R llamada **tidyverse** y un puñado de otros paquetes.
Los paquetes son las unidades fundamentales del código de R reproducible.
Incluyen funciones reutilizables, documentación que describe cómo usarlas y datos de muestra.

### R

Para descargar R, vaya a CRAN la red comprensiva de archivos de R (por sus siglas en inglés: comprehensive R archive network).
CRAN está compuesto por un conjunto de servidores espejo distribuidos por todo el mundo y se utiliza para distribuir R y paquetes de R.
No elijas un espejo cerca de ti; en su lugar, use el espejo de la nube, <https://cloud.r-project.org>, que lo resuelve automáticamente por ti.

Una nueva versión principal de R sale una vez al año y hay 2 o 3 lanzamientos menores cada año.
Es una buena idea actualizar regularmente.
La actualización puede ser un poco complicada, especialmente para las versiones principales que requieren que vuelva a instalar todos sus paquetes, pero posponerlo solo empeora las cosas.
Necesitarás al menos R 4.1.0 para este libro.

### RStudio

RStudio es un entorno de desarrollo integrado, o IDE, para la programación con R.
Descárgalo e instálalo desde <https://posit.co/download/rstudio-desktop/>.
RStudio se actualiza un par de veces al año.
Cuando haya una nueva versión disponible, RStudio se lo hará saber.
Es una buena idea actualizar regularmente para aprovechar las últimas y mejores funciones.
Para este libro, asegúrese de tener al menos RStudio 2022.02.0.

Cuando inicie RStudio, @fig-rstudio-console, verá dos regiones clave en la interfaz: el panel de la consola y el panel de salida.
Por ahora, todo lo que necesita saber es que escribe el código R en el panel de la consola y presiona enter para ejecutarlo.
¡Aprenderás más a medida que avancemos!

```{r}
#| label: fig-rstudio-console
#| echo: false
#| out-width: ~
#| fig-cap: >
#|   El IDE de RStudio tiene dos regiones clave: escriba el código R en el panel de la consola
#|   a la izquierda y busque gráficos en el panel de salida a la derecha.
#| fig-alt: >
#|   El IDE de RStudio con los paneles Consola y Salida resaltados.
knitr::include_graphics("diagrams/rstudio/console.png", dpi = 270)
```

### tidyverse

También deberá instalar algunos paquetes de R.
Un **paquete** de R es una colección de funciones, datos y documentación que amplía las capacidades de R básico.
El uso de paquetes es clave para el uso exitoso de R.
La mayoría de los paquetes que aprenderás en este libro forman parte del llamado tidyverse.
Todos los paquetes en tidyverse comparten una filosofía común de programación de datos y R y están diseñados para trabajar juntos de forma natural.

Puedes instalar el tidyverse completo con una sola línea de código:

```{r}
#| eval: false

install.packages("tidyverse")
```

En su computadora, escriba esa línea de código en la consola y luego presione Intro para ejecutarlo.
R descargará los paquetes de CRAN y los instalará en su computadora.
Si tiene problemas para instalar, asegúrese de estar conectado a Internet y de que <https://cloud.r-project.org/> no está bloqueado por su firewall o proxy.

No podrá usar las funciones, objetos o archivos de ayuda en un paquete hasta que lo cargue con `library()`.
Una vez que haya instalado un paquete, puede cargarlo usando la función `library()`:

```{r}
library(tidyverse)
```

Esto le dice que tidyverse carga nueve paquetes: dplyr, forcats, ggplot2, lubridate, purrr, readr, stringr, tibble, tidyr.
Estos se consideran el **núcleo** del tidyverse porque los usará en casi todos los análisis.

Los paquetes en tidyverse cambian con bastante frecuencia.
Puede comprobar si hay actualizaciones disponibles y, opcionalmente, instalarlas ejecutando `tidyverse_update()`.

### Otros paquetes

Hay muchos otros paquetes excelentes que no forman parte de tidyverse porque resuelven problemas en un dominio diferente o están diseñados con un conjunto diferente de principios subyacentes.
Esto no los hace mejores o peores, simplemente diferentes.
En otras palabras, el complemento del tidyverse no es el desordenado sino muchos otros universos de paquetes interrelacionados.
A medida que aborde más proyectos de ciencia de datos con R, aprenderá nuevos paquetes y nuevas formas de pensar sobre los datos.

Usaremos muchos paquetes de fuera del tidyverse en este libro.
Por ejemplo, utilizamos los siguientes cuatro paquetes de datos para proporcionar aplicaciones interesantes:

```{r}
#| eval: false

install.packages(c("babynames", "gapminder", "nycflights13", "palmerpenguins"))
```

También usaremos una selección de otros paquetes para ejemplos únicos.
No necesita instalarlos ahora, solo recuerde que cada vez que vea un error como este:

```{r}
#| eval: false

library(ggrepel)
#> Error in library(ggrepel) : there is no package called ‘ggrepel’
```

Debe ejecutar `install.packages("ggrepel")` para instalar el paquete.

## Ejecutando código de R

La sección anterior le mostró varios ejemplos de ejecución de código R.
El código en el libro se ve así:

```{r}
#| eval: true
1 + 2
```

Si ejecuta el mismo código en su consola local, se verá así:

    > 1 + 2
    [1] 3

Hay dos diferencias principales.
En su consola, escribe después de `>`, llamado **prompt**; no mostramos el aviso en el libro.
En el libro, la salida se comenta con `#>`; en su consola, aparece directamente después de su código.
Estas dos diferencias significan que si está trabajando con una versión electrónica del libro, puede copiar fácilmente el código del libro y colocarlo en la consola.

A lo largo del libro, usamos un conjunto consistente de convenciones para referirnos al código:

-   Las funciones se muestran en una fuente de código y van seguidas de paréntesis, como `sum()` o `mean()`.

-   Otros objetos R (como datos o argumentos de funciones) están en una fuente de código, sin paréntesis, como `flights` o `x`.

-   A veces, para dejar claro de qué paquete proviene un objeto, usaremos el nombre del paquete seguido de dos puntos, como `dplyr::mutate()` o\
    `nycflights13::flights`.
    Este también es un código R válido.

## Reconocimientos

Este libro no es solo el producto de Hadley, Mine y Garrett, sino que es el resultado de muchas conversaciones (en persona y en línea) que hemos tenido con muchas personas en la comunidad de R.
Hay algunas personas a las que nos gustaría agradecer en particular porque han pasado muchas horas respondiendo nuestras preguntas y ayudándonos a pensar mejor sobre la ciencia de datos:

-   Jenny Bryan y Lionel Henry por muchas discusiones útiles sobre el trabajo con listas y columnas de lista.

-   Los tres capítulos sobre el flujo de trabajo fueron adaptados (con permiso) de <https://stat545.com/block002_hello-r-workspace-wd-project.html> por Jenny Bryan.

-   Yihui Xie por su trabajo con el paquete [bookdown](https://github.com/rstudio/bookdown) y por responder incansablemente a mis solicitudes de funciones.

-   Bill Behrman por su atenta lectura de todo el libro y por probarlo con su clase de ciencia de datos en Stanford.

-   La comunidad de Twitter #rstats que revisó todos los borradores de los capítulos y brindó toneladas de comentarios útiles.

Este libro fue escrito abiertamente y muchas personas contribuyeron con solicitudes de incorporación de cambios para solucionar problemas menores.
Un agradecimiento especial a todos los que contribuyeron a través de GitHub:

```{r}
#| eval: false
#| echo: false

library(tidyverse)
contribs_all_json <- gh::gh("/repos/:owner/:repo/contributors",
  owner = "hadley",
  repo = "r4ds",
  .limit = Inf
)
contribs_all <- tibble(
  login = contribs_all_json %>% map_chr("login"),
  n = contribs_all_json %>% map_int("contributions")
)

contribs_old <- read_csv("contributors.csv", col_types = list())
contribs_new <- contribs_all %>% anti_join(contribs_old, by = "login")

# Get info for new contributors
needed_json <- map(
  contribs_new$login, 
  ~ gh::gh("/users/:username", username = .x),
  .progress = TRUE
)
info_new <- tibble(
  login = contribs_new$login,
  name = map_chr(needed_json, "name", .default = NA),
  blog = map_chr(needed_json, "blog", .default = NA)
)
info_old <- contribs_old %>% select(login, name, blog)
info_all <- bind_rows(info_old, info_new)

contribs_all <- contribs_all %>% 
  left_join(info_all, by = "login") %>% 
  arrange(login)
write_csv(contribs_all, "contributors.csv")
```

```{r}
#| results: asis
#| echo: false
#| message: false

library(dplyr)
contributors <- readr::read_csv("contributors.csv")
contributors <- contributors %>% 
  filter(!login %in% c("hadley", "garrettgman", "mine-cetinkaya-rundel")) %>% 
  mutate(
    login = paste0("\\@", login),
    desc = ifelse(is.na(name), login, paste0(name, " (", login, ")"))
  )

cat("Un gran agradecimiento a todos las ", nrow(contributors), " personas que contribuyeron con mejoras específicas a través de solicitudes de extracción de GitHub (en orden alfabético por nombre de usuario):", sep = "")
cat(paste0(contributors$desc, collapse = ", "))
cat(".\n")
```

## Colofón

Una versión en línea de este libro está disponible en <https://r4ds.hadley.nz>.
Continuará evolucionando entre las reimpresiones del libro físico.
El código fuente del libro está disponible en <https://github.com/hadley/r4ds>.
El libro funciona con [Quarto](https://quarto.org), lo que facilita la escritura de libros que combinan texto y código ejecutable.

Este libro fue construido con:

```{r}
#| echo: false
#| results: asis

pkgs <- sessioninfo::package_info(
  tidyverse:::tidyverse_packages(),
  dependencies = FALSE
)
df <- tibble(
  package = pkgs$package,
  version = pkgs$ondiskversion,
  source = gsub("@", "\\\\@", pkgs$source)
)
knitr::kable(df, format = "markdown")
```

```{r}
#| include: false

cli:::ruler()
```
