# Números {#sec-numbers}

```{r}
#| results: "asis"
#| echo: false
source("_common.R")
status("complete")
```

## Introducción

Los vectores numéricos son la columna vertebral de la ciencia de datos y ya los ha usado varias veces anteriormente en el libro.
Ahora es el momento de examinar sistemáticamente lo que puede hacer con ellos en R, asegurándose de estar bien situado para abordar cualquier problema futuro que involucre vectores numéricos.

Comenzaremos brindándole un par de herramientas para hacer números si tiene cadenas, y luego entraremos en un poco más de detalle de `count()`.
Luego nos sumergiremos en varias transformaciones numéricas que combinan bien con `mutate()`, incluidas transformaciones más generales que se pueden aplicar a otros tipos de vectores, pero que a menudo se usan con vectores numéricos.
Terminaremos cubriendo las funciones de resumen que combinan bien con `summarize()` y le mostraremos cómo también se pueden usar con `mutate()`.

### Requisitos previos

Este capítulo utiliza principalmente funciones de base R, que están disponibles sin cargar ningún paquete.
Pero aún necesitamos el tidyverse porque usaremos estas funciones básicas de R dentro de las funciones de tidyverse como `mutate()` y `filter()`.
Como en el último capítulo, usaremos ejemplos reales de nycflights13, así como ejemplos de juguetes hechos con `c()` y `tribble()`.

```{r}
#| label: setup
#| message: false

library(tidyverse)
library(nycflights13)
```

## Haciendo úmeros

En la mayoría de los casos, obtendrá números ya registrados en uno de los tipos numéricos de R: entero o doble.
En algunos casos, sin embargo, los encontrará como cadenas, posiblemente porque los creó al girar desde los encabezados de columna o porque algo salió mal en su proceso de importación de datos.

readr proporciona dos funciones útiles para analizar cadenas en números: `parse_double()` y `parse_number()`.
Usa `parse_double()` cuando tengas números escritos como cadenas:

```{r}
x <- c("1.2", "5.6", "1e3")
parse_double(x)
```

Usa `parse_number()` cuando la cadena contenga texto no numérico que quieras ignorar.
Esto es particularmente útil para datos de moneda y porcentajes:

```{r}
x <- c("$1,234", "USD 3,513", "59%")
parse_number(x)
```

## Contar

Es sorprendente la cantidad de ciencia de datos que puede hacer con solo conteos y un poco de aritmética básica, por lo que dplyr se esfuerza por hacer que contar sea lo más fácil posible con `count()`.
Esta función es excelente para realizar exploraciones y comprobaciones rápidas durante el análisis:

```{r}
flights |> count(dest)
```

(A pesar de los consejos en @sec-workflow-style, generalmente colocamos `count()` en una sola línea porque generalmente se usa en la consola para verificar rápidamente que un cálculo funciona como se esperaba.)

Si desea ver los valores más comunes, agregue `sort = TRUE`:

```{r}
flights |> count(dest, sort = TRUE)
```

Y recuerda que si quieres ver todos los valores, puedes usar `|> View()` o `|> print(n = Inf)`.

Puede realizar el mismo cálculo "a mano" con `group_by()`, `summarize()` y `n()`.
Esto es útil porque le permite calcular otros resúmenes al mismo tiempo:

```{r}
flights |> 
  group_by(dest) |> 
  summarize(
    n = n(),
    delay = mean(arr_delay, na.rm = TRUE)
  )
```

`n()` es una función de resumen especial que no toma ningún argumento y en su lugar accede a información sobre el grupo "actual".
Esto significa que solo funciona dentro de los verbos dplyr:

```{r}
#| error: true

n()
```

Hay un par de variantes de `n()` y `count()` que pueden resultarle útiles:

-   `n_distinct(x)` cuenta el número de valores distintos (únicos) de una o más variables.
    Por ejemplo, podríamos averiguar qué destinos son atendidos por la mayoría de los transportistas:

    ```{r}
    flights |> 
      group_by(dest) |> 
      summarize(carriers = n_distinct(carrier)) |> 
      arrange(desc(carriers))
    ```

-   Una cuenta ponderada es una suma.
    Por ejemplo, podría "contar" el número de millas que voló cada avión:

    ```{r}
    flights |> 
      group_by(tailnum) |> 
      summarize(miles = sum(distance))
    ```

    Los recuentos ponderados son un problema común, por lo que `count()` tiene un argumento `wt` que hace lo mismo:

    ```{r}
    #| results: false
    flights |> count(tailnum, wt = distance)
    ```

-   Puede contar los valores perdidos combinando `sum()` y `is.na()`.
    En el conjunto de datos de `flights`, esto representa los vuelos que se cancelan:

    ```{r}
    flights |> 
      group_by(dest) |> 
      summarize(n_cancelled = sum(is.na(dep_time))) 
    ```

### Ejercicios

1.  ¿Cómo puedes usar `count()` para contar las filas de números con un valor faltante para una variable dada?
2.  Expanda las siguientes llamadas a `count()` para usar en su lugar `group_by()`, `summarize()` y `arrange()`:
    1.  `flights |> count(dest, sort = TRUE)`

    2.  `flights |> count(tailnum, wt = distance)`

## Transformaciones numéricas

Las funciones de transformación funcionan bien con `mutate()` porque su salida tiene la misma longitud que la entrada.
La gran mayoría de las funciones de transformación ya están integradas en la base R.
No es práctico enumerarlos todos, por lo que esta sección mostrará los más útiles.
Como ejemplo, aunque R proporciona todas las funciones trigonométricas con las que podría soñar, no las enumeramos aquí porque rara vez se necesitan para la ciencia de datos.

### Reglas aritméticas y de reciclaje. {#sec-recycling}

Introdujimos los conceptos básicos de aritmética (`+`, `-`, `*`, `/`, `^`) en @sec-workflow-basics y los hemos usado mucho desde entonces.
Estas funciones no necesitan una gran cantidad de explicación porque hacen lo que aprendiste en la escuela primaria.
Pero necesitamos hablar brevemente sobre las **reglas de reciclaje** que determinan lo que sucede cuando los lados izquierdo y derecho tienen diferentes longitudes.
Esto es importante para operaciones como `flights |> mutate(air_time = air_time / 60)` porque hay 336.776 números a la izquierda de `/` pero solo uno a la derecha.

R maneja las longitudes que no coinciden **reciclando** o repitiendo el vector corto.
Podemos ver esto en funcionamiento más fácilmente si creamos algunos vectores fuera de un data frame:

```{r}
x <- c(1, 2, 10, 20)
x / 5
# is shorthand for
x / c(5, 5, 5, 5)
```

En general, solo desea reciclar números individuales (es decir, vectores de longitud 1), pero R reciclará cualquier vector de longitud más corta.
Por lo general (pero no siempre) le da una advertencia si el vector más largo no es un múltiplo del más corto:

```{r}
x * c(1, 2)
x * c(1, 2, 3)
```

Estas reglas de reciclaje también se aplican a las comparaciones lógicas (`==`, `<`, `<=`, `>`, `>=`, `!=`) y pueden conducir a un resultado sorprendente si accidentalmente usa `==` en lugar de `%in%` y el marco de datos tiene un número desafortunado de filas.
Por ejemplo, tome este código que intenta encontrar todos los vuelos en enero y febrero:

```{r}
flights |> 
  filter(month == c(1, 2))
```

El código se ejecuta sin errores, pero no devuelve lo que desea.
Debido a las reglas de reciclaje, encuentra vuelos en filas impares que partieron en enero y vuelos en filas pares que partieron en febrero.
Y, lamentablemente, no hay ninguna advertencia porque `flights` tiene un número par de filas.

Para protegerlo de este tipo de fallas silenciosas, la mayoría de las funciones de tidyverse utilizan una forma más estricta de reciclaje que solo recicla valores únicos.
Desafortunadamente, eso no ayuda aquí, ni en muchos otros casos, porque el cálculo clave lo realiza la función base R `==`, no `filter()`.

### Mínimo y máximo

Las funciones aritméticas trabajan con pares de variables.
Dos funciones estrechamente relacionadas son `pmin()` y `pmax()`, que cuando se les dan dos o más variables devolverán el valor más pequeño o más grande en cada fila:

```{r}
df <- tribble(
  ~x, ~y,
  1,  3,
  5,  2,
  7, NA,
)

df |> 
  mutate(
    min = pmin(x, y, na.rm = TRUE),
    max = pmax(x, y, na.rm = TRUE)
  )
```

Tenga en cuenta que estas son diferentes a las funciones de resumen `min()` y `max()` que toman múltiples observaciones y devuelven un solo valor.
Puedes darte cuenta de que has usado la forma incorrecta cuando todos los mínimos y todos los máximos tienen el mismo valor:

```{r}
df |> 
  mutate(
    min = min(x, y, na.rm = TRUE),
    max = max(x, y, na.rm = TRUE)
  )
```

### Aritmética modular

La aritmética modular es el nombre técnico del tipo de matemática que hacías antes de aprender sobre los lugares decimales, es decir, la división que produce un número entero y un resto.
En R, `%/%` realiza la división de enteros y `%%` calcula el resto:

```{r}
1:10 %/% 3
1:10 %% 3
```

La aritmética modular es útil para el conjunto de datos `flights`, porque podemos usarla para desempaquetar la variable `sched_dep_time` en `hour` y `minute`:

```{r}
flights |> 
  mutate(
    hour = sched_dep_time %/% 100,
    minute = sched_dep_time %% 100,
    .keep = "used"
  )
```

Podemos combinar eso con el truco `mean(is.na(x))` de @sec-logical-summaries para ver cómo varía la proporción de vuelos cancelados a lo largo del día.
Los resultados se muestran en @fig-prop-cancelled.

```{r}
#| label: fig-prop-cancelled
#| fig-cap: > 
#|   Un gráfico de líneas con la hora de salida programada en el eje x y la proporción
#|   de vuelos cancelados en el eje y. Las cancelaciones parecen acumularse
#|   en el transcurso del día hasta las 8:00 p. m., los vuelos muy tardíos son mucho
#|   menos probables de ser cancelado.
#| fig-alt: >
#|   Un gráfico de líneas que muestra cómo cambia la proporción de vuelos cancelados a lo largo de
#|   el transcurso del día. La proporción comienza baja en torno al 0,5% en
#|   6 a.m., luego aumenta constantemente a lo largo del día hasta alcanzar su punto máximo
#|   el 4% a las 19 h. La proporción de vuelos cancelados cae rápidamente
#|   bajando a alrededor del 1% para la medianoche.

flights |> 
  group_by(hour = sched_dep_time %/% 100) |> 
  summarize(prop_cancelled = mean(is.na(dep_time)), n = n()) |> 
  filter(hour > 1) |> 
  ggplot(aes(x = hour, y = prop_cancelled)) +
  geom_line(color = "grey50") + 
  geom_point(aes(size = n))
```

### Logaritmos

Los logaritmos son una transformación increíblemente útil para manejar datos que varían en varios órdenes de magnitud y convertir el crecimiento exponencial en crecimiento lineal.
En R, puede elegir entre tres logaritmos: `log()` (el logaritmo natural, base e), `log2()` (base 2) y `log10()` (base 10).
Recomendamos usar `log2()` o `log10()`.
`log2()` es fácil de interpretar porque una diferencia de 1 en la escala logarítmica corresponde a duplicar la escala original y una diferencia de -1 corresponde a reducir a la mitad; mientras que `log10()` es fácil de transformar porque (por ejemplo) 3 es 10\^3 = 1000.
El inverso de `log()` es `exp()`; para calcular el inverso de `log2()` o `log10()` necesitará usar `2^` o `10^`.

### Redondeo {#sec-rounding}

Usa `round(x)` para redondear un número al entero más cercano:

```{r}
round(123.456)
```

Puede controlar la precisión del redondeo con el segundo argumento dígitos, `digits`.
`round(x, digits)` se redondea al `10^-n` más cercano, por lo que `digits = 2` se redondea al 0,01 más cercano.
Esta definición es útil porque implica que `round(x, -3)` se redondeará al millar más cercano, lo que de hecho sucede:

```{r}
round(123.456, 2)  # dos dígitos
round(123.456, 1)  # un dígito
round(123.456, -1) # redondear a la decena más cercana
round(123.456, -2) # redondear a la centena más cercana
```

Hay una rareza con `round()` que parece sorprendente a primera vista:

```{r}
round(c(1.5, 2.5))
```

`round()` utiliza lo que se conoce como "redondear la mitad a par" o redondeo bancario: si un número está a medio camino entre dos enteros, se redondeará al entero **par**.
Esta es una buena estrategia porque mantiene el redondeo imparcial: la mitad de todos los 0,5 se redondean hacia arriba y la otra mitad hacia abajo.

`round()` se empareja con `floor()` que siempre redondea hacia abajo y `ceiling()` que siempre redondea hacia arriba:

```{r}
x <- 123.456

floor(x)
ceiling(x)
```

Estas funciones no tienen un argumento dígitos, `digits`, por lo que puede reducir, redondear y luego volver a aumentar:

```{r}
# Redondear hacia abajo a los dos dígitos más cercanos
floor(x / 0.01) * 0.01
# Redondea hacia arriba a los dos dígitos más cercanos
ceiling(x / 0.01) * 0.01
```

Puedes usar la misma técnica si quieres `round()` a un múltiplo de algún otro número:

```{r}
# Redondea al múltiplo más cercano de 4
round(x / 4) * 4

# Redondear al 0,25 más cercano
round(x / 0.25) * 0.25
```

### Cortar números en rangos

Use `cut()`[^numbers-1] para dividir (también conocido como bin) un vector numérico en cubos discretos:

[^numbers-1]: ggplot2 proporciona algunos ayudantes para casos comunes en `cut_interval()`, `cut_number()` y `cut_width()`.
    ggplot2 es un lugar ciertamente extraño para que vivan estas funciones, pero son útiles como parte del cálculo del histograma y se escribieron antes de que existieran otras partes del tidyverse.

```{r}
x <- c(1, 2, 5, 10, 15, 20)
cut(x, breaks = c(0, 5, 10, 15, 20))
```

Los cortes no necesitan estar espaciados uniformemente:

```{r}
cut(x, breaks = c(0, 5, 10, 100))
```

Opcionalmente, puede proporcionar sus propias etiquetas, `labels`.
Tenga en cuenta que debe haber una etiqueta, `labels`, menos que rupturas, `breaks`.

```{r}
cut(x, 
  breaks = c(0, 5, 10, 15, 20), 
  labels = c("sm", "md", "lg", "xl")
)
```

Cualquier valor fuera del rango de las rupturas se convertirá en `NA`:

```{r}
y <- c(NA, -10, 5, 10, 30)
cut(y, breaks = c(0, 5, 10, 15, 20))
```

Consulte la documentación para ver otros argumentos útiles como `right` e `include.lowest`, que controlan si los intervalos son `[a, b)` o `(a, b]` y si el intervalo más bajo debe ser `[a, b]`.

### Agregados acumulativos y rodantes {#sec-cumulative-and-rolling-aggregates}

Base R proporciona `cumsum()`, `cumprod()`, `cummin()`, `cummax()` para ejecutar, o acumular, sumas, productos, mínimos y máximos.
dplyr proporciona `cummean()` para medios acumulativos.
Las sumas acumulativas tienden a ser las más importantes en la práctica:

```{r}
x <- 1:10
cumsum(x)
```

Si necesita agregados rodantes o deslizantes más complejos, pruebe el paquete [slider](https://davisvaughan.github.io/slider/) de Davis Vaughan.

### Ejercicios

1.  Explique con palabras qué hace cada línea del código utilizado para generar @fig-prop-cancelled.

2.  ¿Qué funciones trigonométricas proporciona R?
    Adivina algunos nombres y busca la documentación.
    ¿Usan grados o radianes?

3.  Actualmente, `dep_time` y `sched_dep_time` son convenientes de ver, pero difíciles de calcular porque en realidad no son números continuos.
    Puede ver el problema básico ejecutando el siguiente código: hay un intervalo entre cada hora.

    ```{r}
    #| eval: false
    flights |> 
      filter(month == 1, day == 1) |> 
      ggplot(aes(x = sched_dep_time, y = dep_delay)) +
      geom_point()
    ```

    Conviértalos a una representación más veraz del tiempo (ya sean horas fraccionarias o minutos desde la medianoche).

4.  Redondea `dep_time` y `arr_time` a los cinco minutos más cercanos.

## Transformaciones generales

The following sections describe some general transformations which are often used with numeric vectors, but can be applied to all other column types.

### Ranks

dplyr provides a number of ranking functions inspired by SQL, but you should always start with `dplyr::min_rank()`.
It uses the typical method for dealing with ties, e.g. 1st, 2nd, 2nd, 4th.

```{r}
x <- c(1, 2, 2, 3, 4, NA)
min_rank(x)
```

Note that the smallest values get the lowest ranks; use `desc(x)` to give the largest values the smallest ranks:

```{r}
min_rank(desc(x))
```

If `min_rank()` doesn't do what you need, look at the variants `dplyr::row_number()`, `dplyr::dense_rank()`, `dplyr::percent_rank()`, and `dplyr::cume_dist()`.
See the documentation for details.

```{r}
df <- tibble(x = x)
df |> 
  mutate(
    row_number = row_number(x),
    dense_rank = dense_rank(x),
    percent_rank = percent_rank(x),
    cume_dist = cume_dist(x)
  )
```

You can achieve many of the same results by picking the appropriate `ties.method` argument to base R's `rank()`; you'll probably also want to set `na.last = "keep"` to keep `NA`s as `NA`.

`row_number()` can also be used without any arguments when inside a dplyr verb.
In this case, it'll give the number of the "current" row.
When combined with `%%` or `%/%` this can be a useful tool for dividing data into similarly sized groups:

```{r}
df <- tibble(id = 1:10)

df |> 
  mutate(
    row0 = row_number() - 1,
    three_groups = row0 %% 3,
    three_in_each_group = row0 %/% 3
  )
```

### Offsets

`dplyr::lead()` and `dplyr::lag()` allow you to refer the values just before or just after the "current" value.
They return a vector of the same length as the input, padded with `NA`s at the start or end:

```{r}
x <- c(2, 5, 11, 11, 19, 35)
lag(x)
lead(x)
```

-   `x - lag(x)` gives you the difference between the current and previous value.

    ```{r}
    x - lag(x)
    ```

-   `x == lag(x)` tells you when the current value changes.

    ```{r}
    x == lag(x)
    ```

You can lead or lag by more than one position by using the second argument, `n`.

### Consecutive identifiers

Sometimes you want to start a new group every time some event occurs.
For example, when you're looking at website data, it's common to want to break up events into sessions, where you begin a new session after gap of more than `x` minutes since the last activity.
For example, imagine you have the times when someone visited a website:

```{r}
events <- tibble(
  time = c(0, 1, 2, 3, 5, 10, 12, 15, 17, 19, 20, 27, 28, 30)
)

```

And you've computed the time between each event, and figured out if there's a gap that's big enough to qualify:

```{r}
events <- events |> 
  mutate(
    diff = time - lag(time, default = first(time)),
    has_gap = diff >= 5
  )
events
```

But how do we go from that logical vector to something that we can `group_by()`?
`cumsum()`, from @sec-cumulative-and-rolling-aggregates, comes to the rescue as gap, i.e. `has_gap` is `TRUE`, will increment `group` by one (@sec-numeric-summaries-of-logicals):

```{r}
events |> mutate(
  group = cumsum(has_gap)
)
```

Another approach for creating grouping variables is `consecutive_id()`, which starts a new group every time one of its arguments changes.
For example, inspired by [this stackoverflow question](https://stackoverflow.com/questions/27482712), imagine you have a data frame with a bunch of repeated values:

```{r}
df <- tibble(
  x = c("a", "a", "a", "b", "c", "c", "d", "e", "a", "a", "b", "b"),
  y = c(1, 2, 3, 2, 4, 1, 3, 9, 4, 8, 10, 199)
)
```

If you want to keep the first row from each repeated `x`, you could use `group_by()`, `consecutive_id()`, and `slice_head()`:

```{r}
df |> 
  group_by(id = consecutive_id(x)) |> 
  slice_head(n = 1)
```

### Exercises

1.  Find the 10 most delayed flights using a ranking function.
    How do you want to handle ties?
    Carefully read the documentation for `min_rank()`.

2.  Which plane (`tailnum`) has the worst on-time record?

3.  What time of day should you fly if you want to avoid delays as much as possible?

4.  What does `flights |> group_by(dest) |> filter(row_number() < 4)` do?
    What does `flights |> group_by(dest) |> filter(row_number(dep_delay) < 4)` do?

5.  For each destination, compute the total minutes of delay.
    For each flight, compute the proportion of the total delay for its destination.

6.  Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave.
    Using `lag()`, explore how the average flight delay for an hour is related to the average delay for the previous hour.

    ```{r}
    #| results: false

    flights |> 
      mutate(hour = dep_time %/% 100) |> 
      group_by(year, month, day, hour) |> 
      summarize(
        dep_delay = mean(dep_delay, na.rm = TRUE),
        n = n(),
        .groups = "drop"
      ) |> 
      filter(n > 5)
    ```

7.  Look at each destination.
    Can you find flights that are suspiciously fast (i.e. flights that represent a potential data entry error)?
    Compute the air time of a flight relative to the shortest flight to that destination.
    Which flights were most delayed in the air?

8.  Find all destinations that are flown by at least two carriers.
    Use those destinations to come up with a relative ranking of the carriers based on their performance for the same destination.

## Numeric summaries

Just using the counts, means, and sums that we've introduced already can get you a long way, but R provides many other useful summary functions.
Here is a selection that you might find useful.

### Center

So far, we've mostly used `mean()` to summarize the center of a vector of values.
Because the mean is the sum divided by the count, it is sensitive to even just a few unusually high or low values.
An alternative is to use the `median()`, which finds a value that lies in the "middle" of the vector, i.e. 50% of the values is above it and 50% are below it.
Depending on the shape of the distribution of the variable you're interested in, mean or median might be a better measure of center.
For example, for symmetric distributions we generally report the mean while for skewed distributions we usually report the median.

@fig-mean-vs-median compares the mean vs. the median when looking at the hourly vs. median departure delay for each destination.
The median delay is always smaller than the mean delay because flights sometimes leave multiple hours late, but never leave multiple hours early.

```{r}
#| label: fig-mean-vs-median
#| fig-cap: >
#|   A scatterplot showing the differences of summarizing hourly depature
#|   delay with median instead of mean.
#| fig-alt: >
#|   All points fall below a 45° line, meaning that the median delay is
#|   always less than the mean delay. Most points are clustered in a 
#|   dense region of mean [0, 20] and median [0, 5]. As the mean delay
#|   increases, the spread of the median also increases. There are two
#|   outlying points with mean ~60, median ~50, and mean ~85, median ~55.
flights |>
  group_by(year, month, day) |>
  summarize(
    mean = mean(dep_delay, na.rm = TRUE),
    median = median(dep_delay, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) |> 
  ggplot(aes(x = mean, y = median)) + 
  geom_abline(slope = 1, intercept = 0, color = "white", linewidth = 2) +
  geom_point()
```

You might also wonder about the **mode**, or the most common value.
This is a summary that only works well for very simple cases (which is why you might have learned about it in high school), but it doesn't work well for many real datasets.
If the data is discrete, there may be multiple most common values, and if the data is continuous, there might be no most common value because every value is ever so slightly different.
For these reasons, the mode tends not to be used by statisticians and there's no mode function included in base R[^numbers-2].

[^numbers-2]: The `mode()` function does something quite different!

### Minimum, maximum, and quantiles {#sec-min-max-summary}

What if you're interested in locations other than the center?
`min()` and `max()` will give you the largest and smallest values.
Another powerful tool is `quantile()` which is a generalization of the median: `quantile(x, 0.25)` will find the value of `x` that is greater than 25% of the values, `quantile(x, 0.5)` is equivalent to the median, and `quantile(x, 0.95)` will find the value that's greater than 95% of the values.

For the `flights` data, you might want to look at the 95% quantile of delays rather than the maximum, because it will ignore the 5% of most delayed flights which can be quite extreme.

```{r}
flights |>
  group_by(year, month, day) |>
  summarize(
    max = max(dep_delay, na.rm = TRUE),
    q95 = quantile(dep_delay, 0.95, na.rm = TRUE),
    .groups = "drop"
  )
```

### Spread

Sometimes you're not so interested in where the bulk of the data lies, but in how it is spread out.
Two commonly used summaries are the standard deviation, `sd(x)`, and the inter-quartile range, `IQR()`.
We won't explain `sd()` here since you're probably already familiar with it, but `IQR()` might be new --- it's `quantile(x, 0.75) - quantile(x, 0.25)` and gives you the range that contains the middle 50% of the data.

We can use this to reveal a small oddity in the `flights` data.
You might expect the spread of the distance between origin and destination to be zero, since airports are always in the same place.
But the code below makes it looks like one airport, [EGE](https://en.wikipedia.org/wiki/Eagle_County_Regional_Airport), might have moved.

```{r}
flights |> 
  group_by(origin, dest) |> 
  summarize(
    distance_sd = IQR(distance), 
    n = n(),
    .groups = "drop"
  ) |> 
  filter(distance_sd > 0)
```

### Distributions

It's worth remembering that all of the summary statistics described above are a way of reducing the distribution down to a single number.
This means that they're fundamentally reductive, and if you pick the wrong summary, you can easily miss important differences between groups.
That's why it's always a good idea to visualize the distribution before committing to your summary statistics.

@fig-flights-dist shows the overall distribution of departure delays.
The distribution is so skewed that we have to zoom in to see the bulk of the data.
This suggests that the mean is unlikely to be a good summary and we might prefer the median instead.

```{r}
#| echo: false
#| label: fig-flights-dist
#| fig-cap: >
#|   (Left) The histogram of the full data is extremely skewed making it
#|   hard to get any details. (Right) Zooming into delays of less than two
#|   hours makes it possible to see what's happening with the bulk of the
#|   observations.
#| fig-alt: >
#|   Two histograms of `dep_delay`. On the left, it's very hard to see
#|   any pattern except that there's a very large spike around zero, the
#|   bars rapidly decay in height, and for most of the plot, you can't
#|   see any bars because they are too short to see. On the right,
#|   where we've discarded delays of greater than two hours, we can
#|   see that the spike occurs slightly below zero (i.e. most flights
#|   leave a couple of minutes early), but there's still a very steep
#|   decay after that.
#| fig-asp: 0.5
library(patchwork)

full <- flights |>
  ggplot(aes(x = dep_delay)) + 
  geom_histogram(binwidth = 15, na.rm = TRUE)

delayed120 <- flights |>
  filter(dep_delay < 120) |> 
  ggplot(aes(x = dep_delay)) + 
  geom_histogram(binwidth = 5)

full + delayed120
```

It's also a good idea to check that distributions for subgroups resemble the whole.
@fig-flights-dist-daily overlays a frequency polygon for each day.
The distributions seem to follow a common pattern, suggesting it's fine to use the same summary for each day.

```{r}
#| label: fig-flights-dist-daily
#| fig-cap: >
#|   365 frequency polygons of `dep_delay`, one for each day. The frequency
#|   polygons appear to have the same shape, suggesting that it's reasonable
#|   to compare days by looking at just a few summary statistics.
#| fig-alt: >
#|   The distribution of `dep_delay` is highly right skewed with a strong
#|   peak slightly less than 0. The 365 frequency polygons are mostly 
#|   overlapping forming a thick black bland.
flights |>
  filter(dep_delay < 120) |> 
  ggplot(aes(x = dep_delay, group = interaction(day, month))) + 
  geom_freqpoly(binwidth = 5, alpha = 1/5)
```

Don't be afraid to explore your own custom summaries specifically tailored for the data that you're working with.
In this case, that might mean separately summarizing the flights that left early vs. the flights that left late, or given that the values are so heavily skewed, you might try a log-transformation.
Finally, don't forget what you learned in @sec-sample-size: whenever creating numerical summaries, it's a good idea to include the number of observations in each group.

### Positions

There's one final type of summary that's useful for numeric vectors, but also works with every other type of value: extracting a value at a specific position: `first(x)`, `last(x)`, and `nth(x, n)`.

For example, we can find the first and last departure for each day:

```{r}
flights |> 
  group_by(year, month, day) |> 
  summarize(
    first_dep = first(dep_time, na_rm = TRUE), 
    fifth_dep = nth(dep_time, 5, na_rm = TRUE),
    last_dep = last(dep_time, na_rm = TRUE)
  )
```

(NB: Because dplyr functions use `_` to separate components of function and arguments names, these functions use `na_rm` instead of `na.rm`.)

If you're familiar with `[`, which we'll come back to in @sec-subset-many, you might wonder if you ever need these functions.
There are three reasons: the `default` argument allows you to provide a default if the specified position doesn't exist, the `order_by` argument allows you to locally override the order of the rows, and the `na_rm` argument allows you to drop missing values.

Extracting values at positions is complementary to filtering on ranks.
Filtering gives you all variables, with each observation in a separate row:

```{r}
flights |> 
  group_by(year, month, day) |> 
  mutate(r = min_rank(desc(sched_dep_time))) |> 
  filter(r %in% c(1, max(r)))
```

### With `mutate()`

As the names suggest, the summary functions are typically paired with `summarize()`.
However, because of the recycling rules we discussed in @sec-recycling they can also be usefully paired with `mutate()`, particularly when you want do some sort of group standardization.
For example:

-   `x / sum(x)` calculates the proportion of a total.
-   `(x - mean(x)) / sd(x)` computes a Z-score (standardized to mean 0 and sd 1).
-   `(x - min(x)) / (max(x) - min(x))` standardizes to range \[0, 1\].
-   `x / first(x)` computes an index based on the first observation.

### Exercises

1.  Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights.
    When is `mean()` useful?
    When is `median()` useful?
    When might you want to use something else?
    Should you use arrival delay or departure delay?
    Why might you want to use data from `planes`?

2.  Which destinations show the greatest variation in air speed?

3.  Create a plot to further explore the adventures of EGE.
    Can you find any evidence that the airport moved locations?

## Summary

You're already familiar with many tools for working with numbers, and after reading this chapter you now know how to use them in R.
You've also learned a handful of useful general transformations that are commonly, but not exclusively, applied to numeric vectors like ranks and offsets.
Finally, you worked through a number of numeric summaries, and discussed a few of the statistical challenges that you should consider.

Over the next two chapters, we'll dive into working with strings with the stringr package.
Strings are a big topic so they get two chapters, one on the fundamentals of strings and one on regular expressions.
